---
title: "Performance Optimization"
subtitle: "Micro-optimizations on modern CPUs"
format:
  revealjs: 
    slide-number: true
    theme: moon
    preview-links: auto
    logo: images/fluxim.png
    embed-resources: true
    output-dir: docs
resources:
  - performance_optimization.pdf
---

## Easyperf
![](images/cover.jpg){height="400"}


::: aside
Easyperf blog, book and labs available [here](https://easyperf.net/).
:::


# High-level analysis

## Flame graph
![](images/flame_graph.PNG){fig-align="center"}

# CPU cache and memory access

## Basic cache architecture
::: columns
::: {.column width="35%"}
   * all memory access through cache
   * L1/L2 cache per core
   * L3 cache shared between cores
   * NUMA adds another level
:::

::: {.column width="3%"}
:::

::: {.column width="52%"}
![](images/zen3_cache.png)
:::
:::

## Set-associative cache
::: columns
::: {.column width="45%"}
   * hierarchical access
   * cache line 64 bytes
   * set idx = address mod #sets
   * cache lines can conflict
:::

::: {.column width="3%"}
:::

::: {.column width="52%"}
![](images/setassociative_cropped.jpeg.png)
:::
:::

## AMD Ryzen 7 5850U

```{python}
import numpy as np
units = {
  'B': 1,
  'kB': 1024,
  'MB': 1048576
}

allsizes = {}
allrates = {}

for line in open("myoutput.txt", "r"):
   if line == "\n":
      continue
   values = line.split(",")
   key = values[0] # this is the plot legend
   msstring = values[1].split(" ")
   unit = msstring[4]
   memsize = float(msstring[3]) * units[unit] # in bytes
   rate = float(values[3].split(" ")[1]) # in MB/s

   if key not in allsizes:
      #allsizes[key] = [memsize]
      #allrates[key] = [rate]
      allsizes[key] = np.array([memsize])
      allrates[key] = np.array([rate * units['MB']])
   else:
      #allsizes[key].append(memsize)
      #allrates[key].append(rate)
      allsizes[key] = np.append(allsizes[key], [memsize])
      allrates[key] = np.append(allrates[key], [rate * units['MB']])
```

```{python}
import matplotlib.pyplot as plt

fig, ax = plt.subplots()

yunit = 1024*1024*1024

ax.plot(allsizes['Sequential read (64-bit)'], allrates['Sequential read (64-bit)']/yunit, '-bo', label = "MOV (64 bit)")
ax.plot(allsizes['Sequential read (128-bit)'], allrates['Sequential read (128-bit)']/yunit, '-rs', label = "MOVDQA (SSE2, 128 bit)")
ax.plot(allsizes['Sequential read (256-bit)'], allrates['Sequential read (256-bit)']/yunit, '-g*', label = "VMOVDQA (AVX, 256 bit)")
ax.plot(allsizes['Random read (64-bit)'], allrates[ 'Random read (64-bit)']/yunit, '--b')
ax.plot(allsizes['Random read (128-bit)'], allrates['Random read (128-bit)']/yunit, '--r')
ax.plot(allsizes['Random read (256-bit)'], allrates['Random read (256-bit)']/yunit, '--g')

ax.set_xscale('log')

ax.set_xlabel('Data size [B]')
ax.set_ylabel('Transfer rate [GB/s]')
#plt.line(allsizes['Random read (64-bit)'], allrates[ 'Random read (64-bit)']/yunit, color = "blue", line_dash='dashed')
#plt.circle(allsizes['Random read (64-bit)'], allrates[ 'Random read (64-bit)']/yunit, color = "blue", line_dash='dashed')
#plt.line(allsizes['Random read (128-bit)'], allrates['Random read (128-bit)']/yunit, color = "red", line_dash='dashed')
#plt.diamond(allsizes['Random read (128-bit)'], allrates['Random read (128-bit)']/yunit, color = "red", line_dash='dashed')
#plt.line(allsizes['Random read (256-bit)'], allrates['Random read (256-bit)']/yunit, color = "green", line_dash='dashed')
#plt.square(allsizes['Random read (256-bit)'], allrates['Random read (256-bit)']/yunit, color = "green", line_dash='dashed')

# plot cache sizes
l1cache = 32*1024
l2cache = 512*1024
l3cache = 16*1024*1024
ax.fill_between([1e2, l1cache, l1cache, 1e2], [0, 0, 270, 270], alpha=0.5, facecolor="grey")
ax.fill_between([l1cache, l2cache, l2cache, l1cache], [0, 0, 270, 270], alpha=0.5, facecolor="lightgrey")
ax.fill_between([l2cache, l3cache, l3cache, l2cache], [0, 0, 270, 270], alpha=0.5, facecolor="darkgrey")


plt.legend()

plt.show()
```

::: aside
Bandwidth measurements were taken using [bandwidth](https://zsmith.co/bandwidth.php)
:::

## Common pitfalls

* random access prevents pre-fetching
* strided access pollutes cache lines
* hyper threading potentially pollutes cache lines
* different thread writes to my cache line ("false sharing")
* memory ordering in C++ (sequential consistency..)


# CPU micro architecture

## Pipeline model
Increased throughput by pipeline.

::: columns
::: {.column width="55%"}
   1. Instruction fetch (IF)
   2. Instruction decode (ID)
   3. Execute (EXE)
   4. Memory access (MEM)
   5. Write back (WRI)
:::

::: {.column width="3%"}
:::

::: {.column width="32%"}
![](images/pipeline.png){fig-align="center" width=1600}
:::
:::

Execution is [superscalar]{style="color: red;"} and [out-of-order]{style="color: red;"}.

## Latency of common operations
![](images/latency.png){fig-align="center" height="400"}

Latency: Time before next [dependent]{style="color: red;"} instruction is issued.

## Common pitfalls

* Long dependency chains prevent out-of-order exection
* Port contention
* Branch misprediction
* Aliasing prevents loop optimizations
* Missed vectorization opportunities

# Roofline analysis

## Basic idea
Roofline plot: Plot arithmetic intensity vs. FLOP/s

::: columns
::: {.column width="45%"}

```{=tex}
\begin{equation}
\mathrm{AI} = \frac{\mathrm{FLOP}}{\mathrm{byte}}
\end{equation}
```
:::

::: {.column width="3%"}
:::

::: {.column width="42%"}
![](images/Roofline-intro.png){fig-align="center"}
:::
:::

Premise: AI is algorithm property and hardware-independent (not true!)

## Roofline analysis - continued
::: columns
::: {.column width="42%"}
![](images/Roofline-memory.png){fig-align="center"  style="background-color:white;" width="600" }
:::

::: {.column width="6%"}
:::

::: {.column width="42%"}
![](images/Roofline-CPU.jpg){fig-align="center" width="600"}
:::
:::

Rooflines depend on implementation of algorithm!

## Example
```{.cpp code-line-numbers="6"}
void multiply(float *A, float *B, float *C) {
   for (size_t i = 0; i < N; ++i) {
      for (size_t j =0; j < N; ++j) {
         for (size_t k = 0; k < N; ++k) {
            // very naughty memory access pattern!
            C[i*N+j] += A[i*N+k] * B[k*N+j];
         }
      }
   }
}
```

Neglecting integer operations, we have

```{=tex}
\begin{equation}
\mathrm{AI} = \frac{2}{3}
\end{equation}
```

## My machine
```{python}
import numpy as np
seqread64L1 = np.average(np.array(allrates['Sequential read (64-bit)'])[np.array(allsizes['Sequential read (64-bit)']) < 1e4])
seqread64L2 = np.average(np.array(allrates['Sequential read (64-bit)'])[np.logical_and(np.array(allsizes['Sequential read (64-bit)']) > 6e4, np.array(allsizes['Sequential read (64-bit)']) < 2e5)])
seqread64L3 = np.average(np.array(allrates['Sequential read (64-bit)'])[np.logical_and(np.array(allsizes['Sequential read (64-bit)']) > 1e6, np.array(allsizes['Sequential read (64-bit)']) < 1e7)])
seqread64DDR = np.average(np.array(allrates['Sequential read (64-bit)'])[np.array(allsizes['Sequential read (64-bit)']) > 2e7])

randread64L1 = np.average(np.array(allrates['Random read (64-bit)'])[np.array(allsizes['Random read (64-bit)']) < 1e4])
randread64L2 = np.average(np.array(allrates['Random read (64-bit)'])[np.logical_and(np.array(allsizes['Random read (64-bit)']) > 6e4, np.array(allsizes['Random read (64-bit)']) < 2e5)])
randread64L3 = np.average(np.array(allrates['Random read (64-bit)'])[np.logical_and(np.array(allsizes['Random read (64-bit)']) > 1e6, np.array(allsizes['Random read (64-bit)']) < 1e7)])
randread64DDR = np.average(np.array(allrates['Random read (64-bit)'])[np.array(allsizes['Random read (64-bit)']) > 2e7])
```

```{python}
fig, ax = plt.subplots()

ax.set_xscale('log')
ax.set_yscale('log')

x = 2.0**np.mgrid[-2:9]
y64l1 = seqread64L1 * x/yunit/4
y64DDR = seqread64DDR * x/yunit/4
y64rl1 = randread64L1 * x/yunit/4
y64rDDR = randread64DDR * x/yunit/4
ax.plot(x, y64l1, 'b-', label = "Sequential L1")
ax.plot(x, y64DDR, 'g-', label = "Sequential DDR")
ax.plot(x, y64rl1, 'b--', label = "Random L1")
ax.plot(x, y64rDDR, 'g--', label = "Random DDR")

# add peak performance
pi1 = 128 * np.ones_like(x)
pi0 = 4 * np.ones_like(x)
ax.plot(x, pi1, 'r-', label = "Vectorized peak")
ax.plot(x, pi0, 'r--', label = "Scalar peak")

ax.plot([0.75, 0.75], [0.1, 1e4], "k-", label ="GEMM")

ax.set_xlabel('Computational intensity [FLOP/B]')
ax.set_ylabel('Performance [GFLOP/s]')

plt.legend(loc="upper left")
plt.show()
```

::: aside
Peak FLOP measurements were taken using [peakperf](https://github.com/Dr-Noob/peakperf)
:::

## setfos
![](images/roofline_setfos.png){fig-align="center"}


# Top-Down Microarchitecture Analysis

## Performance metrics
::: columns
::: {.column width="65%"}
* Retired instructions: instructions that were executed and not thrown away (speculative instructions)
* Cycles per instruction (CPI)
* Instructions per cycle (IPC)
* Pipeline slot occupation
* Cache missses

:::

::: {.column width="3%"}
:::

::: {.column width="22%"}
![](images/PipelineSlot.jpg){fig-align="center"}
:::
:::

## Top-Down Microarchitecture Analysis
![](images/TMAM.png){fig-align="center"}

## Setfos VTUNE TMAM
![](images/tmam_vtune.png){fig-align="center"}

